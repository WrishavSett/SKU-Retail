{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13d1686a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from google import genai\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b6c215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "client = genai.Client(api_key=gemini_api_key)\n",
    "\n",
    "model_name = \"gemini-3-flash-preview\"\n",
    "chunk_size = 10\n",
    "max_iterations = 10\n",
    "\n",
    "master_file_path = f\"/home/wrishav/Desktop/Workspace/SKU-Retail/redundant/data_master.csv\"\n",
    "m_columns = ['itemcode', 'catcode', 'category', 'subcat', 'ssubcat', 'company', 'mbrand', 'brand', 'sku',\n",
    "             'packtype', 'base_pack', 'flavor', 'color', 'wght', 'uom', 'mrp']\n",
    "\n",
    "transaction_file_path = f\"/home/wrishav/Desktop/Workspace/SKU-Retail/redundant/data_transaction.csv\"\n",
    "t_columns = ['CATEGORY', 'MANUFACTURE', 'BRAND', 'ITEMDESC', 'MRP', 'PACKSIZE', 'PACKTYPE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68133ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "master_file = pd.read_csv(master_file_path, usecols=m_columns)\n",
    "master_dict = master_file.to_dict(orient='index')\n",
    "\n",
    "transaction_file = pd.read_csv(transaction_file_path, usecols=t_columns)\n",
    "transaction_dict = transaction_file.to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48157535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data for prompt generation\n",
    "def format_master(master_row):\n",
    "    return f\"\"\"\n",
    "Item Code: {master_row['itemcode']}\n",
    "Category Code: {master_row['catcode']}\n",
    "Category: {master_row['category']}\n",
    "Subcategory: {master_row['subcat']}\n",
    "Sub-Subcategory: {master_row['ssubcat']}\n",
    "Company: {master_row['company']}\n",
    "Main Brand: {master_row['mbrand']}\n",
    "Brand: {master_row['brand']}\n",
    "Pack Type: {master_row['packtype']}\n",
    "Pack Size: {master_row['base_pack']}\n",
    "Flavor: {master_row['flavor']}\n",
    "Color: {master_row['color']}\n",
    "Unit of Measure: {master_row['uom']}\n",
    "MRP: {master_row['mrp']}\n",
    "    \"\"\"\n",
    "\n",
    "def format_transaction(transaction_row):\n",
    "    return f\"\"\"\n",
    "Category Code: {transaction_row['CATEGORY']}\n",
    "Company: {transaction_row['MANUFACTURE']}\n",
    "Brand: {transaction_row['BRAND']}\n",
    "Item Description: {transaction_row['ITEMDESC']}\n",
    "MRP: {transaction_row['MRP']}\n",
    "Pack Size: {re.match(r'(\\d+)\\s*(.*)', transaction_row['PACKSIZE']).group(1)}\n",
    "Unit of Measure: {re.match(r'(\\d+)\\s*(.*)', transaction_row['PACKSIZE']).group(2)}\n",
    "Pack Type: {transaction_row['PACKTYPE']}\n",
    "    \"\"\"\n",
    "\n",
    "# Format context data for LLM prompt\n",
    "def format_context_matches(master_dictionary) -> str:\n",
    "    \"\"\"Format context matches for the LLM prompt\"\"\"\n",
    "    context_lines = []\n",
    "    \n",
    "    for idx, master_row in master_dictionary.items():\n",
    "        formatted_row = format_master(master_row)\n",
    "        context_lines.append(f\"Context item {idx}:{formatted_row}\")\n",
    "    return \"\\n\".join(context_lines)\n",
    "\n",
    "# Format query data for LLM prompt\n",
    "def format_query_match(transaction_dictionary) -> str:\n",
    "    \"\"\"Format context matches for the LLM prompt\"\"\"\n",
    "    context_lines = []\n",
    "    \n",
    "    for idx, master_row in transaction_dictionary.items():\n",
    "        formatted_row = format_transaction(master_row)\n",
    "        context_lines.append(f\"Transaction item {idx}:{formatted_row}\")\n",
    "    return \"\\n\".join(context_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07026c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate prompt\n",
    "def generate_prompt(context_rows, transaction_row):\n",
    "  prompt = f\"\"\"\n",
    "You are an expert product matching AI.\n",
    "You have been given a transaction item and a set of context items from the master catalog.\n",
    "Determine which context item best matches the transaction item.\n",
    "\n",
    "Here are the context items from the master catalog:\n",
    "\\n{format_context_matches(context_rows)}\n",
    "\n",
    "Here is the transaction item to match:\n",
    "\\n{format_query_match(transaction_row)}\n",
    "\n",
    "Matching criteria (Priority Order):\n",
    "1. Category code alignment\n",
    "2. EXACT company match\n",
    "3. EXACT brand match\n",
    "4. EXACT pack size and pack type match\n",
    "5. MRP/price similarity\n",
    "\n",
    "Instructions:\n",
    "1. Compare transaction item with each context item carefully\n",
    "2. Prioritize exact matches in category code, company, brand, pack size, and pack type\n",
    "3. Consider MRP similarity as a secondary factor\n",
    "\n",
    "Based on the attributes provided, identify the best matching context item for the transaction item.\n",
    "Respond strictly with a JSON object in the following format:\n",
    "{{\n",
    "  \"context_item\": \"<The context item number, for example, 0 if 'Context item 0'>\",\n",
    "  \"score\": \"<The confidence score normalized between 0 and 1, with 1 being a perfect match>\",\n",
    "}}\n",
    "  \"\"\"\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c641e0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call LLM\n",
    "def call_llm(prompt):\n",
    "    response = client.models.generate_content(\n",
    "        model=model_name,\n",
    "        contents=prompt,\n",
    "        # config=gen_config\n",
    "    )\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76ab0485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0 to 9\n",
      "Chunk start index: 0\n",
      "\n",
      "LLM Response:\n",
      "sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=11>\n",
      ") candidates=[Candidate(\n",
      "  content=Content(\n",
      "    parts=[\n",
      "      Part(\n",
      "        text=\"\"\"```json\n",
      "{\n",
      "  \"context_item\": \"8\",\n",
      "  \"score\": 0.7\n",
      "}\n",
      "```\"\"\",\n",
      "        thought_signature=b'\\x12\\xa3\\xf3\\x01\\n\\x9f\\xf3\\x01\\x01r\\xc8\\xda|\\x93\\xaf\\\\\\xba\\xf2\\xf2\\x01+!\\xcf\\x1d\\xa1\\xdc\\xc6o\\x15\\x9a\\xbe9\\xa9hUv\\xd9\\xea\\xfd\\xcf\\xa2\\'7\\x04Y\\xffa\\xc0\\x02\\xcc\\xdd\\x89/\\xa9pL<\\xc8\\x9394g\\xd3\\x1b\\xf9\\xff\\xfb\\x0b\\xa2:\\x83\\xc5B\\x0ba\\x03\\x89\\xaf\\xfdp\\xfcoQ?}\\x87\\xb0\"\\x05\\x91\\xccJ\\xc6EC\\xfb ...'\n",
      "      ),\n",
      "    ],\n",
      "    role='model'\n",
      "  ),\n",
      "  finish_reason=<FinishReason.STOP: 'STOP'>,\n",
      "  index=0\n",
      ")] create_time=None model_version='gemini-3-flash-preview' prompt_feedback=None response_id='W6hkadHhDIiqjuMP3p3o-AI' usage_metadata=GenerateContentResponseUsageMetadata(\n",
      "  candidates_token_count=27,\n",
      "  prompt_token_count=1416,\n",
      "  prompt_tokens_details=[\n",
      "    ModalityTokenCount(\n",
      "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
      "      token_count=1416\n",
      "    ),\n",
      "  ],\n",
      "  thoughts_token_count=9788,\n",
      "  total_token_count=11231\n",
      ") automatic_function_calling_history=[] parsed=None\n",
      "\n",
      "Processing 10 to 16\n",
      "Chunk start index: 10\n",
      "\n",
      "LLM Response:\n",
      "sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=11>\n",
      ") candidates=[Candidate(\n",
      "  content=Content(\n",
      "    parts=[\n",
      "      Part(\n",
      "        text=\"\"\"{\n",
      "  \"context_item\": \"10\",\n",
      "  \"score\": 0.98\n",
      "}\"\"\",\n",
      "        thought_signature=b'\\x12\\xe7\\x12\\n\\xe4\\x12\\x01r\\xc8\\xda|\\xc0\\xca(\\xb5VJkQ\\x85\\xa1b\\xce\\x82sGn\\xea\\x82\\xfe`\\xd2\\x18nR\\x06r\\xfbR\\x80\\x11\\xc4\\xec\\x1dQ\\x16\\xael\\x83\\xdd\\xee\\xda\\x8e\\x8c\\x8a\\xacJ\\xfa\\xa1\\x17d\\xedruk\\xfeC\\x82\\x11b\\x1cf\\x9ex\\xb8\\x888]h\\x9cm\\xba\\x02\\xbbs\\xdbq\\x11D\\xa1\\xf6?\\xb7\\x03\\xc0\\x9fx...'\n",
      "      ),\n",
      "    ],\n",
      "    role='model'\n",
      "  ),\n",
      "  finish_reason=<FinishReason.STOP: 'STOP'>,\n",
      "  index=0\n",
      ")] create_time=None model_version='gemini-3-flash-preview' prompt_feedback=None response_id='YKhkad_wMZ7mjuMP5bHf2A4' usage_metadata=GenerateContentResponseUsageMetadata(\n",
      "  candidates_token_count=24,\n",
      "  prompt_token_count=1106,\n",
      "  prompt_tokens_details=[\n",
      "    ModalityTokenCount(\n",
      "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
      "      token_count=1106\n",
      "    ),\n",
      "  ],\n",
      "  thoughts_token_count=814,\n",
      "  total_token_count=1944\n",
      ") automatic_function_calling_history=[] parsed=None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chunksize = 10\n",
    "context_len = len(master_dict)\n",
    "responses = []\n",
    "\n",
    "start = 0\n",
    "while start < context_len:\n",
    "    end = min(start + chunksize, context_len)\n",
    "    print(f\"Processing {start} to {end-1}\\nChunk start index: {start}\\n\")\n",
    "    context_rows = {i: master_dict[i] for i in range(start, end)}\n",
    "    transaction_row = {0: transaction_dict[0]}\n",
    "    prompt = generate_prompt(context_rows, transaction_row)\n",
    "    response = call_llm(prompt)\n",
    "    responses.append(response)\n",
    "    print(f\"LLM Response:\\n{response}\\n\")\n",
    "    start += chunksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0bdeabe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[GenerateContentResponse(\n",
       "   automatic_function_calling_history=[],\n",
       "   candidates=[\n",
       "     Candidate(\n",
       "       content=Content(\n",
       "         parts=[\n",
       "           Part(\n",
       "             text=\"\"\"```json\n",
       " {\n",
       "   \"context_item\": \"8\",\n",
       "   \"score\": 0.7\n",
       " }\n",
       " ```\"\"\",\n",
       "             thought_signature=b'\\x12\\xa3\\xf3\\x01\\n\\x9f\\xf3\\x01\\x01r\\xc8\\xda|\\x93\\xaf\\\\\\xba\\xf2\\xf2\\x01+!\\xcf\\x1d\\xa1\\xdc\\xc6o\\x15\\x9a\\xbe9\\xa9hUv\\xd9\\xea\\xfd\\xcf\\xa2\\'7\\x04Y\\xffa\\xc0\\x02\\xcc\\xdd\\x89/\\xa9pL<\\xc8\\x9394g\\xd3\\x1b\\xf9\\xff\\xfb\\x0b\\xa2:\\x83\\xc5B\\x0ba\\x03\\x89\\xaf\\xfdp\\xfcoQ?}\\x87\\xb0\"\\x05\\x91\\xccJ\\xc6EC\\xfb ...'\n",
       "           ),\n",
       "         ],\n",
       "         role='model'\n",
       "       ),\n",
       "       finish_reason=<FinishReason.STOP: 'STOP'>,\n",
       "       index=0\n",
       "     ),\n",
       "   ],\n",
       "   model_version='gemini-3-flash-preview',\n",
       "   response_id='W6hkadHhDIiqjuMP3p3o-AI',\n",
       "   sdk_http_response=HttpResponse(\n",
       "     headers=<dict len=11>\n",
       "   ),\n",
       "   usage_metadata=GenerateContentResponseUsageMetadata(\n",
       "     candidates_token_count=27,\n",
       "     prompt_token_count=1416,\n",
       "     prompt_tokens_details=[\n",
       "       ModalityTokenCount(\n",
       "         modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "         token_count=1416\n",
       "       ),\n",
       "     ],\n",
       "     thoughts_token_count=9788,\n",
       "     total_token_count=11231\n",
       "   )\n",
       " ),\n",
       " GenerateContentResponse(\n",
       "   automatic_function_calling_history=[],\n",
       "   candidates=[\n",
       "     Candidate(\n",
       "       content=Content(\n",
       "         parts=[\n",
       "           Part(\n",
       "             text=\"\"\"{\n",
       "   \"context_item\": \"10\",\n",
       "   \"score\": 0.98\n",
       " }\"\"\",\n",
       "             thought_signature=b'\\x12\\xe7\\x12\\n\\xe4\\x12\\x01r\\xc8\\xda|\\xc0\\xca(\\xb5VJkQ\\x85\\xa1b\\xce\\x82sGn\\xea\\x82\\xfe`\\xd2\\x18nR\\x06r\\xfbR\\x80\\x11\\xc4\\xec\\x1dQ\\x16\\xael\\x83\\xdd\\xee\\xda\\x8e\\x8c\\x8a\\xacJ\\xfa\\xa1\\x17d\\xedruk\\xfeC\\x82\\x11b\\x1cf\\x9ex\\xb8\\x888]h\\x9cm\\xba\\x02\\xbbs\\xdbq\\x11D\\xa1\\xf6?\\xb7\\x03\\xc0\\x9fx...'\n",
       "           ),\n",
       "         ],\n",
       "         role='model'\n",
       "       ),\n",
       "       finish_reason=<FinishReason.STOP: 'STOP'>,\n",
       "       index=0\n",
       "     ),\n",
       "   ],\n",
       "   model_version='gemini-3-flash-preview',\n",
       "   response_id='YKhkad_wMZ7mjuMP5bHf2A4',\n",
       "   sdk_http_response=HttpResponse(\n",
       "     headers=<dict len=11>\n",
       "   ),\n",
       "   usage_metadata=GenerateContentResponseUsageMetadata(\n",
       "     candidates_token_count=24,\n",
       "     prompt_token_count=1106,\n",
       "     prompt_tokens_details=[\n",
       "       ModalityTokenCount(\n",
       "         modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "         token_count=1106\n",
       "       ),\n",
       "     ],\n",
       "     thoughts_token_count=814,\n",
       "     total_token_count=1944\n",
       "   )\n",
       " )]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21a61c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_item': '10', 'score': 0.98}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "text = responses[1].text\n",
    "start = text.find('{')\n",
    "end = text.rfind('}') + 1\n",
    "json_response = json.loads(text[start:end])\n",
    "json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2729b6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n  \"context_item\": \"8\",\\n  \"score\": 0.7\\n}\\n```'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses[0].candidates[0].content.parts[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fd6b62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call LLM\n",
    "def call_llm(prompt):\n",
    "    response = client.models.generate_content(\n",
    "        model=model_name,\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            thinking_config=types.ThinkingConfig(thinking_budget=0), # Disables thinking\n",
    "            system_instruction=\"\"\"\n",
    "                You are a an expert product matching AI.\n",
    "                Return ONLY a valid JSON object that strictly adheres to the specified schema.\n",
    "                \"\"\",\n",
    "            temperature=0.1,\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"context_item\": {\"type\": \"string\"},\n",
    "                    \"score\": {\"type\": \"number\"}\n",
    "                },\n",
    "                \"required\": [\"context_item\", \"score\"]\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d4cb8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0 to 9\n",
      "Chunk start index: 0\n",
      "\n",
      "LLM Response:\n",
      "sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=11>\n",
      ") candidates=[Candidate(\n",
      "  content=Content(\n",
      "    parts=[\n",
      "      Part(\n",
      "        text='{\"context_item\": \"9\", \"score\": 0.1}',\n",
      "        thought_signature=b'\\x124\\n2\\x01r\\xc8\\xda|\\xa1\\xba\\x9b\\x1e\\xd6\\x8d\\x97\\xbe\\x9c\\xb4\\x03\\xb0\\xe1\\x02A\\x06\\x0f\\xd9\\xca\\xc3\\x8d\\xa2\\xbd0\\x94\\x07}\\x0e\\x9d$\\x9c(l\\x82\\x1e\\xc0}\\xe1\\xc7\\xcd\\xe9\\x97\\xef\\xf6\\xb1'\n",
      "      ),\n",
      "    ],\n",
      "    role='model'\n",
      "  ),\n",
      "  finish_reason=<FinishReason.STOP: 'STOP'>,\n",
      "  index=0\n",
      ")] create_time=None model_version='gemini-3-flash-preview' prompt_feedback=None response_id='YqhkafPxE-uJ4-EPwM6W8QI' usage_metadata=GenerateContentResponseUsageMetadata(\n",
      "  candidates_token_count=16,\n",
      "  prompt_token_count=1446,\n",
      "  prompt_tokens_details=[\n",
      "    ModalityTokenCount(\n",
      "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
      "      token_count=1446\n",
      "    ),\n",
      "  ],\n",
      "  total_token_count=1462\n",
      ") automatic_function_calling_history=[] parsed={'context_item': '9', 'score': 0.1}\n",
      "\n",
      "Processing 10 to 16\n",
      "Chunk start index: 10\n",
      "\n",
      "LLM Response:\n",
      "sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=11>\n",
      ") candidates=[Candidate(\n",
      "  content=Content(\n",
      "    parts=[\n",
      "      Part(\n",
      "        text='{\"context_item\":\"10\",\"score\":0.95}',\n",
      "        thought_signature=b'\\x124\\n2\\x01r\\xc8\\xda|2\\x1a\\x921\\xcc\\xb4n\\x13[-\\x04\\x07\\xc3\\x92\\xedL\\x9c*w\\xf2\\xba\\x0bu\\x17\\x0f\\x13\\xb6\\x9d\\xe40E\\x10qD\\xce\\xf0/\\x9b\\xe5\\xf7\\n]\\x9cl_'\n",
      "      ),\n",
      "    ],\n",
      "    role='model'\n",
      "  ),\n",
      "  finish_reason=<FinishReason.STOP: 'STOP'>,\n",
      "  index=0\n",
      ")] create_time=None model_version='gemini-3-flash-preview' prompt_feedback=None response_id='Y6hkadDtIJXljuMPzuSMuQ4' usage_metadata=GenerateContentResponseUsageMetadata(\n",
      "  candidates_token_count=15,\n",
      "  prompt_token_count=1136,\n",
      "  prompt_tokens_details=[\n",
      "    ModalityTokenCount(\n",
      "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
      "      token_count=1136\n",
      "    ),\n",
      "  ],\n",
      "  total_token_count=1151\n",
      ") automatic_function_calling_history=[] parsed={'context_item': '10', 'score': 0.95}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chunksize = 10\n",
    "context_len = len(master_dict)\n",
    "responses = []\n",
    "\n",
    "start = 0\n",
    "while start < context_len:\n",
    "    end = min(start + chunksize, context_len)\n",
    "    print(f\"Processing {start} to {end-1}\\nChunk start index: {start}\\n\")\n",
    "    context_rows = {i: master_dict[i] for i in range(start, end)}\n",
    "    transaction_row = {0: transaction_dict[0]}\n",
    "    prompt = generate_prompt(context_rows, transaction_row)\n",
    "    response = call_llm(prompt)\n",
    "    responses.append(response)\n",
    "    print(f\"LLM Response:\\n{response}\\n\")\n",
    "    start += chunksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13902920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[GenerateContentResponse(\n",
       "   automatic_function_calling_history=[],\n",
       "   candidates=[\n",
       "     Candidate(\n",
       "       content=Content(\n",
       "         parts=[\n",
       "           Part(\n",
       "             text='{\"context_item\": \"9\", \"score\": 0.1}',\n",
       "             thought_signature=b'\\x124\\n2\\x01r\\xc8\\xda|\\xa1\\xba\\x9b\\x1e\\xd6\\x8d\\x97\\xbe\\x9c\\xb4\\x03\\xb0\\xe1\\x02A\\x06\\x0f\\xd9\\xca\\xc3\\x8d\\xa2\\xbd0\\x94\\x07}\\x0e\\x9d$\\x9c(l\\x82\\x1e\\xc0}\\xe1\\xc7\\xcd\\xe9\\x97\\xef\\xf6\\xb1'\n",
       "           ),\n",
       "         ],\n",
       "         role='model'\n",
       "       ),\n",
       "       finish_reason=<FinishReason.STOP: 'STOP'>,\n",
       "       index=0\n",
       "     ),\n",
       "   ],\n",
       "   model_version='gemini-3-flash-preview',\n",
       "   parsed={\n",
       "     'context_item': '9',\n",
       "     'score': 0.1\n",
       "   },\n",
       "   response_id='YqhkafPxE-uJ4-EPwM6W8QI',\n",
       "   sdk_http_response=HttpResponse(\n",
       "     headers=<dict len=11>\n",
       "   ),\n",
       "   usage_metadata=GenerateContentResponseUsageMetadata(\n",
       "     candidates_token_count=16,\n",
       "     prompt_token_count=1446,\n",
       "     prompt_tokens_details=[\n",
       "       ModalityTokenCount(\n",
       "         modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "         token_count=1446\n",
       "       ),\n",
       "     ],\n",
       "     total_token_count=1462\n",
       "   )\n",
       " ),\n",
       " GenerateContentResponse(\n",
       "   automatic_function_calling_history=[],\n",
       "   candidates=[\n",
       "     Candidate(\n",
       "       content=Content(\n",
       "         parts=[\n",
       "           Part(\n",
       "             text='{\"context_item\":\"10\",\"score\":0.95}',\n",
       "             thought_signature=b'\\x124\\n2\\x01r\\xc8\\xda|2\\x1a\\x921\\xcc\\xb4n\\x13[-\\x04\\x07\\xc3\\x92\\xedL\\x9c*w\\xf2\\xba\\x0bu\\x17\\x0f\\x13\\xb6\\x9d\\xe40E\\x10qD\\xce\\xf0/\\x9b\\xe5\\xf7\\n]\\x9cl_'\n",
       "           ),\n",
       "         ],\n",
       "         role='model'\n",
       "       ),\n",
       "       finish_reason=<FinishReason.STOP: 'STOP'>,\n",
       "       index=0\n",
       "     ),\n",
       "   ],\n",
       "   model_version='gemini-3-flash-preview',\n",
       "   parsed={\n",
       "     'context_item': '10',\n",
       "     'score': 0.95\n",
       "   },\n",
       "   response_id='Y6hkadDtIJXljuMPzuSMuQ4',\n",
       "   sdk_http_response=HttpResponse(\n",
       "     headers=<dict len=11>\n",
       "   ),\n",
       "   usage_metadata=GenerateContentResponseUsageMetadata(\n",
       "     candidates_token_count=15,\n",
       "     prompt_token_count=1136,\n",
       "     prompt_tokens_details=[\n",
       "       ModalityTokenCount(\n",
       "         modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "         token_count=1136\n",
       "       ),\n",
       "     ],\n",
       "     total_token_count=1151\n",
       "   )\n",
       " )]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8626660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_item': '10', 'score': 0.95}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "text = responses[1].text\n",
    "start = text.find('{')\n",
    "end = text.rfind('}') + 1\n",
    "json_response = json.loads(text[start:end])\n",
    "json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "025b632b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"context_item\": \"9\", \"score\": 0.1}'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses[0].candidates[0].content.parts[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dce07e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_item': '9', 'score': 0.1}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'context_item': '10', 'score': 0.95}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(responses[0].parsed, responses[1].parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df2d90b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
